# Product Requirements Document (PRD): Precision Meets Efficiency - Quantization in Radiology AI

**Version:** 1.0
**Date:** 2025-04-15

## 1. Introduction

This document defines the requirements for the "Precision Meets Efficiency: A Gentle Introduction to Quantization in Radiology AI" project. The primary goal is to produce a high-quality manuscript accompanied by a functional, reproducible codebase that serves as a practical guide to quantization techniques in the context of radiology AI.

## 2. Goals

*   Educate researchers, clinicians, and engineers on the fundamentals and practical application of quantization in radiology AI.
*   Demonstrate the trade-offs between model performance (accuracy) and efficiency (speed, memory) using concrete benchmarks.
*   Provide a reproducible codebase for others to learn from and potentially adapt.
*   Highlight ethical and practical considerations for deploying quantized models in clinical settings.
*   Produce a manuscript suitable for dissemination (e.g., preprint, journal submission).

## 3. Scope

### 3.1. In Scope

*   **Manuscript:** Covering Abstract, Introduction, Fundamentals (PTQ, QAT, INT8, INT4), Techniques (Tools: `bitsandbytes`, PyTorch Quantization, TFLite; Advanced: LLM.int8(), QLoRA context), Applications (CXR, MRI benchmarks), Ethical/Practical Considerations, Future Directions, Conclusion, References.
*   **Quantization Techniques Covered:** Focus on PTQ (Static), `bitsandbytes` INT8 and NF4 (via loading configurations). QAT mentioned conceptually, implementation optional based on time/complexity.
*   **Benchmarks:**
    *   Chest X-ray Classification: Using a standard dataset (e.g., NIH ChestX-ray14) and model (e.g., DenseNet-121). Comparing FP32 vs. PTQ INT8 vs. `bitsandbytes` INT8.
    *   MRI Brain Segmentation: Using a standard dataset (e.g., BraTS) and model (e.g., 3D U-Net). Comparing FP32 vs. PTQ INT8.
*   **Metrics:** Accuracy (AUC for CXR, Dice for MRI), Inference Latency, Estimated Model Memory Footprint.
*   **Codebase:** Python implementation using PyTorch, `bitsandbytes`, `scikit-learn`, `pandas`, etc. Includes benchmark scripts, common utilities, configuration files.
*   **Reproducibility:** Conda environment file (`environment.yml`), clear setup instructions (`README.md`), configuration files for benchmarks.
*   **Supporting Documents:** CLAIM Checklist (`docs/CLAIM_Checklist.md`), Quantization Readiness Scorecard (`docs/Quantization_Readiness_Scorecard.md`).
*   **Repository:** Public GitHub repository with structure defined previously.

### 3.2. Out of Scope

*   Exhaustive review of *all* existing quantization methods.
*   Benchmarking on *all* possible radiology tasks, datasets, or model architectures.
*   Development of novel quantization algorithms.
*   Building a production-ready, deployable clinical application or UI.
*   Extensive hyperparameter optimization for base models (use standard pre-trained or baseline configurations).
*   Support for frameworks other than PyTorch and conceptually TensorFlow Lite (implementation focus on PyTorch).

## 4. Target Audience

*   AI Researchers in Medical Imaging
*   Machine Learning Engineers working on healthcare applications
*   Radiologists and Clinicians interested in AI deployment challenges
*   Students learning about model optimization techniques

## 5. Requirements

### 5.1. Manuscript Requirements

*   **MR-1:** All sections defined in the outline (Abstract to Conclusion) must be present and populated according to the provided abstract/outline content.
*   **MR-2:** The manuscript must be written in clear, accessible language suitable for the target audience.
*   **MR-3:** Benchmark results presented in the Applications section must accurately reflect the outputs generated by the codebase.
*   **MR-4:** Claims made (e.g., % reduction in memory, speedup factor) must be substantiated by benchmark results.
*   **MR-5:** The References section (`bibliography.bib`) must be comprehensive and correctly formatted.
*   **MR-6:** All figures and tables referenced must be included and clearly labelled.

### 5.2. Code Requirements

*   **CR-1:** Provide a `environment.yml` file to create a Conda environment capable of running all code.
*   **CR-2:** Implement `code/benchmarks/common/quantization_wrappers.py` with functions for applying PyTorch PTQ INT8 and loading models with `bitsandbytes` INT8/NF4 configurations.
*   **CR-3:** Implement `code/benchmarks/common/evaluation_metrics.py` with functions to calculate AUC, Dice coefficient, inference time, and estimate model memory.
*   **CR-4 (CXR):** Implement `code/benchmarks/cxr_classification/` including:
    *   `data_loader_cxr.py`: Load specified dataset (e.g., ChestX-ray14) from path defined in config.
    *   `model_densenet.py`: Provide DenseNet-121 model structure compatible with pre-trained weights and quantization wrappers.
    *   `run_cxr_benchmark.py`: Orchestrate data loading, model loading (FP32, PTQ, BNB), evaluation, and saving results based on `config_cxr.yaml`.
    *   `config_cxr.yaml`: Allow configuration of dataset path, methods, batch size, output dir.
*   **CR-5 (MRI):** Implement `code/benchmarks/mri_segmentation/` including:
    *   `data_loader_mri.py`: Load specified 3D dataset (e.g., BraTS) from path defined in config.
    *   `model_unet.py`: Provide 3D U-Net model structure compatible with quantization wrappers.
    *   `run_mri_benchmark.py`: Orchestrate data loading, model loading (FP32, PTQ), evaluation (Dice), and saving results based on `config_mri.yaml`.
    *   `config_mri.yaml`: Allow configuration of dataset path, methods, output dir.
*   **CR-6:** Code must be reasonably commented and follow basic Python best practices (e.g., PEP8).
*   **CR-7:** Benchmark scripts must save processed results to CSV files in the `results/processed/` directory.
*   **CR-8:** The main `README.md` must provide clear instructions for setup and running benchmarks.

### 5.3. Supporting Document Requirements

*   **DR-1:** Complete the `docs/CLAIM_Checklist.md` by mapping manuscript/code components to CLAIM criteria.
*   **DR-2:** Complete the `docs/Quantization_Readiness_Scorecard.md` template with explanatory details.

## 6. Success Metrics

*   Manuscript draft completed covering all specified sections.
*   Codebase successfully runs both CXR and MRI benchmarks, producing results consistent with manuscript claims.
*   Benchmark results are saved in the specified format.
*   Conda environment can be successfully created from `environment.yml`.
*   Setup and execution instructions in `README.md` are accurate and lead to successful runs.
*   CLAIM Checklist and Scorecard documents are completed.
*   (Optional) Positive feedback from internal review or peer review if submitted.

## 7. Future Considerations / Open Questions

*   Explore QAT implementation if PTQ shows significant degradation.
*   Benchmark `bitsandbytes` NF4 quantization if INT8 results are promising and hardware permits.
*   Investigate joint optimization techniques (pruning + quantization).
*   Add more detailed error handling and logging to scripts.
